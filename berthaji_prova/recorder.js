import {
    preEmphasis,
    frameSignal,
    applyWindow,
    fft,
    melFilterBank,
    logCompress,
    dct
} from './mfccUtils.js';


export class CommandRecorder {
    constructor() {
        this.audioContext = new (window.AudioContext || window.webkitAudioContext)();
        this.commands = {};
        this.mediaStream = null;
        this.processor = null;
        this.collectedBuffers = [];  // Buffer per accumulare l'audio
    }

    async recordCommand(label) { // Registra qualcosa
        console.log("üé§ Inizio acquisizione stream audio");

        if (this.audioContext.state === 'suspended') {
            await this.audioContext.resume();
            console.log("üé§ AudioContext avviato");
        }

        try {
            console.log("üé§ Accesso al microfono...");
            this.mediaStream = await navigator.mediaDevices.getUserMedia({ audio: true }); // Accede al microfono
            const source = this.audioContext.createMediaStreamSource(this.mediaStream);

            console.log("üé§ Microfono attivato");

            await this.audioContext.audioWorklet.addModule("audio-processor.js");
            console.log("üé§ Audio Worklet caricato correttamente");

            this.processor = new AudioWorkletNode(this.audioContext, "audio-processor");

            // Crea un GainNode per monitorare il flusso audio
            const gainNode = this.audioContext.createGain();

            // Collega la sorgente audio al GainNode
            source.connect(gainNode);
            console.log("üîó Sorgente audio collegata al GainNode");

            // Collega il GainNode al preprocessore
            gainNode.connect(this.processor);
            console.log("üîó GainNode collegato al processore");

            // Collega il preprocessore alla destinazione (output audio)
            this.processor.connect(this.audioContext.destination);
            console.log("üîó Processore collegato alla destinazione");

            // Invia il messaggio per impostare la modalit√† di registrazione
            this.processor.port.postMessage({ mode: "record", label });

            console.log(`üé§ Parla ora per registrare il comando: "${label}"`); // Registra quello che dici

            this.collectedBuffers = []; // Pulisce i dati precedenti

            return new Promise((resolve) => {
                this.processor.port.onmessage = (event) => {
                    const audioBuffer = event.data.buffer;

                    if (audioBuffer && audioBuffer.length > 0) {
                        //console.log(`üéµ Dati audio ricevuti: ${audioBuffer.length} campioni`);
                        this.collectedBuffers.push(...audioBuffer); // Accumula i dati audio
                    } else {
                        console.log("‚ö†Ô∏è Nessun dato audio ricevuto");
                    }
                };

                // Quando la registrazione viene fermata, processa l'audio
                this.stopRecording = () => {
                    console.log("‚èπÔ∏è Registrazione fermata manualmente.");

                    if (this.collectedBuffers.length > 0) {
                        // Filtra il silenzio iniziale e finale nell'audio catturato
                        const processedBuffer = trimSilence(this.collectedBuffers, 0.01);
                        console.log(`üîç Audio dopo rimozione silenzio: ${processedBuffer.length} campioni`);

                        if (processedBuffer.length > 0) {
                            this.commands[label] = this.extractMFCC(processedBuffer); // Estrai 13 caratteristiche (APPROSSIMAZIONE)
                            console.log(`‚úÖ Comando "${label}" registrato con ${processedBuffer.length} campioni`);
                        } else {
                            console.log("‚ö†Ô∏è Nessun audio valido dopo il filtraggio.");
                        }
                    } else {
                        console.log("‚ö†Ô∏è Nessun audio registrato.");
                    }

                    // Spegni il microfono rilasciando lo stream
                    if (this.mediaStream) {
                        this.mediaStream.getTracks().forEach(track => track.stop());
                        console.log("üé§ Microfono disattivato.");
                        this.mediaStream = null;
                        this.processor.disconnect();
                        console.log("üé§ Processore audio disconnesso.");
                        this.processor.disconnect();
                        gainNode.disconnect();
                        source.disconnect();  // Disconnetti anche la sorgente
                        this.processor.port.onmessage = null; // Rimuove l'event listener
                    }
                    resolve();
                };
            });

        } catch (error) {
            console.error("‚ö†Ô∏è Errore durante la registrazione del comando:", error);
        }
    }

    extractMFCC(audioBuffer) {
        console.log("üîä Estrazione MFCC");

        // Verifica se il buffer √® valido
        if (!audioBuffer || audioBuffer.length === 0) {
            console.log("‚ö†Ô∏è Buffer audio vuoto, non √® possibile estrarre MFCC.");
            return [];
        }

        // Ottieni il sampleRate dal contesto audio
        const sampleRate = this.audioContext.sampleRate;
        console.log(`üé∂ Sample rate: ${sampleRate}`);

        // Stampa la lunghezza del buffer e un'anteprima dei primi 20 valori
        console.log(`üìè Lunghezza buffer audio prima di estrarre MFCC: ${audioBuffer.length}`);
        console.log(`üîç Anteprima dei primi 20 campioni del buffer:", ${audioBuffer.slice(0, 20)}`);

        // 1. Pre-emphasis (filtro)
        const emphasizedSignal = preEmphasis(audioBuffer);
        console.log(`üîç Segnale dopo pre-emphasis: ${emphasizedSignal.slice(0, 20)}`);

        // 2. Frame del segnale
        const frameSize = 1024;
        const hopSize = 512;
        const frames = frameSignal(emphasizedSignal, frameSize, hopSize);
        console.log(`üîç Numero di frame ${frames.length}`);

        // 3. Applicare la finestra di Hamming ad ogni frame
        const windowedFrames = frames.map(frame => applyWindow(frame));

        // 4. Calcolare la FFT per ogni frame e applicare il filtro Mel
        const melFilters = melFilterBank(26, frameSize, sampleRate);  // 26 filtri Mel
        const melSpectrogram = windowedFrames.map(frame => {
            const fftResult = fft(frame);
            const magnitudeSpectrum = fftResult.slice(0, frameSize / 2).map(x => Math.sqrt(x.real * x.real + x.imag * x.imag));
            const melSpectrum = melFilters.map(filter => filter.map(f => magnitudeSpectrum[f] || 0).reduce((acc, curr) => acc + curr, 0));
            return logCompress(melSpectrum); // Compress the mel spectrum logarithmically
        });

        // 5. Estrazione dei MFCC con DCT (Discrete Cosine Transform)
        const mfccs = melSpectrogram.map(mel => dct(mel).slice(0, 13)); // Estrarre i primi 13 coefficienti

        // Stampa i risultati per debug
        console.log(`üîç MFCC estratti: ${mfccs}`);
        return mfccs;
    }

    getCommands() {
        return this.commands;
    }
}

// Funzione per rimuovere il silenzio iniziale e finale
function trimSilence(buffer, threshold) {
    let start = 0;
    let end = buffer.length - 1;

    // Trova il primo punto con suono significativo
    while (start < buffer.length && Math.abs(buffer[start]) < threshold) {
        start++;
    }

    // Trova l'ultimo punto con suono significativo
    while (end > 0 && Math.abs(buffer[end]) < threshold) {
        end--;
    }

    const trimmedBuffer = start < end ? buffer.slice(start, end + 1) : [];
    console.log(`üî™ Buffer audio dopo rimozione silenzio: ${trimmedBuffer.length} campioni`);
    return trimmedBuffer;
}